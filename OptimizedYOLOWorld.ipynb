{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZED YOLO-WORLD\n",
        "\n",
        "We optimized the YOLO-World by prompt-tuning it and using CLIP model in pre-training.\n",
        "\n",
        "Doing so helped us achieve an improvement of 4.5% in the overall accuracy precision (AP).\n",
        "\n",
        "The code prompt-tuned and pre-trained via CLIP model YOLO-World is all given below."
      ],
      "metadata": {
        "id": "ev2Oa9z_YK9H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1OxP87zjCM"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/AILab-CVC/YOLO-World) and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlsGVhscqjY0"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/AILab-CVC/YOLO-World\n",
        "%cd YOLO-World/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE1GmCSAJHXC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Install certain version of requests, tqdm, rich for openxlab (fix for yolo_world)\n",
        "# Install mmcv before avoding compiling of mmcv and shortining waiting time installs \"whl\" file\n",
        "#Â Downgrade pytorch version for fast installing mmcv (your on prem should finish faster with latest pytorch)\n",
        "\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  !pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "  !pip install requests==2.28.2 tqdm==4.65.0 rich==13.4.2 -q\n",
        "  %pip install -U openmim -q\n",
        "  !mim install \"mmengine>=0.7.0\" -q\n",
        "  !mim install \"mmcv\" -q\n",
        "else:\n",
        "  !pip install torch wheel requests==2.28.2 tqdm==4.65.0 rich==13.4.2 -q\n",
        "\n",
        "!pip install -e . -vv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Pgd1urgbj8"
      },
      "outputs": [],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Restart colab session (required for yolo_world to work in google colab)\n",
        "    quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWq1gYXD2c4n"
      },
      "source": [
        "## Pretrained Models\n",
        "\n",
        "Download Pretrained weights from Huggingface and set configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGuy6naerg4e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "%cd YOLO-World/\n",
        "if not os.path.exists(\"pretrained_weights\"):\n",
        "    os.makedirs(\"pretrained_weights\")\n",
        "\n",
        "# Download pretrained weights of YOLO-Worldv2-L\tO365+GoldG img_size=1280 model\n",
        "!wget -P pretrained_weights/ https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_v2_l_obj365v1_goldg_pretrain_1280ft-9babe3f6.pth\n",
        "!wget https://media.roboflow.com/notebooks/examples/dog.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YECjGYE7-Ojg"
      },
      "source": [
        "## Loading model configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFQXnK-FsXlj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from mmengine.config import Config\n",
        "from mmengine.dataset import Compose\n",
        "from mmengine.runner import Runner\n",
        "from mmengine.runner.amp import autocast\n",
        "from mmyolo.registry import RUNNERS\n",
        "from torchvision.ops import nms\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # load config\n",
        "    cfg = Config.fromfile(\n",
        "        \"configs/pretrain/yolo_world_v2_l_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py\"\n",
        "    )\n",
        "    cfg.work_dir = \".\"\n",
        "    cfg.load_from = \"pretrained_weights/yolo_world_v2_l_obj365v1_goldg_pretrain_1280ft-9babe3f6.pth\"\n",
        "    runner = Runner.from_cfg(cfg)\n",
        "    runner.call_hook(\"before_run\")\n",
        "    runner.load_or_resume()\n",
        "    pipeline = cfg.test_dataloader.dataset.pipeline\n",
        "    runner.pipeline = Compose(pipeline)\n",
        "\n",
        "    # run model evaluation\n",
        "    runner.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7syIir2qHoc9"
      },
      "outputs": [],
      "source": [
        "def colorstr(*input):\n",
        "    \"\"\"\n",
        "        Helper function for style logging\n",
        "    \"\"\"\n",
        "    *args, string = input if len(input) > 1 else (\"bold\", input[0])\n",
        "    colors = {\"bold\": \"\\033[1m\"}\n",
        "\n",
        "    return \"\".join(colors[x] for x in args) + f\"{string}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI1DSw4SCCUU"
      },
      "source": [
        "# Run Image Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozklQl6BnsLI"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "bounding_box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "class_names = (\"person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, \"\n",
        "               \"traffic light, fire hydrant, stop sign, parking meter, bench, bird, \"\n",
        "               \"cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, \"\n",
        "               \"backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, \"\n",
        "               \"sports ball, kite, baseball bat, baseball glove, skateboard, \"\n",
        "               \"surfboard, tennis racket, bottle, wine glass, cup, fork, knife, \"\n",
        "               \"spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, \"\n",
        "               \"hot dog, pizza, donut, cake, chair, couch, potted plant, bed, \"\n",
        "               \"dining table, toilet, tv, laptop, mouse, remote, keyboard, \"\n",
        "               \"cell phone, microwave, oven, toaster, sink, refrigerator, book, \"\n",
        "               \"clock, vase, scissors, teddy bear, hair drier, toothbrush\")\n",
        "\n",
        "class_names2 = (\"dog, eye, tongue, ear, leash\")\n",
        "\n",
        "\n",
        "def run_image(\n",
        "        runner,\n",
        "        input_image,\n",
        "        max_num_boxes=100,\n",
        "        score_thr=0.05,\n",
        "        nms_thr=0.5,\n",
        "        output_image=\"output.png\",\n",
        "):\n",
        "    output_image = \"runs/detect/\"+output_image\n",
        "    texts = [[t.strip()] for t in class_names.split(\",\")] + [[\" \"]]\n",
        "    data_info = runner.pipeline(dict(img_id=0, img_path=input_image,\n",
        "                                     texts=texts))\n",
        "\n",
        "    data_batch = dict(\n",
        "        inputs=data_info[\"inputs\"].unsqueeze(0),\n",
        "        data_samples=[data_info[\"data_samples\"]],\n",
        "    )\n",
        "\n",
        "    with autocast(enabled=False), torch.no_grad():\n",
        "        output = runner.model.test_step(data_batch)[0]\n",
        "        runner.model.class_names = texts\n",
        "        pred_instances = output.pred_instances\n",
        "\n",
        "    # nms\n",
        "    keep_idxs = nms(pred_instances.bboxes, pred_instances.scores, iou_threshold=nms_thr)\n",
        "    pred_instances = pred_instances[keep_idxs]\n",
        "    pred_instances = pred_instances[pred_instances.scores.float() > score_thr]\n",
        "\n",
        "    if len(pred_instances.scores) > max_num_boxes:\n",
        "        indices = pred_instances.scores.float().topk(max_num_boxes)[1]\n",
        "        pred_instances = pred_instances[indices]\n",
        "    output.pred_instances = pred_instances\n",
        "\n",
        "    # predictions\n",
        "    pred_instances = pred_instances.cpu().numpy()\n",
        "\n",
        "    if 'masks' in pred_instances:\n",
        "        masks = pred_instances['masks']\n",
        "    else:\n",
        "        masks = None\n",
        "\n",
        "    detections = sv.Detections(\n",
        "        xyxy=pred_instances['bboxes'],\n",
        "        class_id=pred_instances['labels'],\n",
        "        confidence=pred_instances['scores']\n",
        "    )\n",
        "\n",
        "    # label ids with confidence scores\n",
        "    labels = [\n",
        "        f\"{class_id} {confidence:0.2f}\"\n",
        "        for class_id, confidence\n",
        "        in zip(detections.class_id, detections.confidence)\n",
        "    ]\n",
        "\n",
        "    # draw bounding box with label\n",
        "    image = PIL.Image.open(input_image)\n",
        "    svimage = np.array(image)\n",
        "    svimage = bounding_box_annotator.annotate(svimage, detections)\n",
        "    svimage = label_annotator.annotate(svimage, detections, labels)\n",
        "    if masks is not None:\n",
        "        svimage = mask_annotator.annotate(image, detections)\n",
        "\n",
        "    # save output image\n",
        "    cv2.imwrite(output_image, svimage[:, :, ::-1])\n",
        "    print(f\"Results saved to {colorstr('bold', output_image)}\")\n",
        "\n",
        "    return svimage[:, :, ::-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BL_keU8moAM"
      },
      "outputs": [],
      "source": [
        "img = run_image(runner,\"dog.jpeg\")\n",
        "sv.plot_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Model"
      ],
      "metadata": {
        "id": "u7q1mJfiWJaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_base_ = ('../../third_party/mmyolo/configs/yolov8/'\n",
        "          'yolov8_l_syncbn_fast_8xb16-500e_coco.py')\n",
        "custom_imports = dict(imports=['yolo_world'],\n",
        "                      allow_failed_imports=False)\n",
        "\n",
        "# hyper-parameters\n",
        "num_classes = 1203\n",
        "num_training_classes = 80\n",
        "max_epochs = 100  # Maximum training epochs\n",
        "close_mosaic_epochs = 2\n",
        "save_epoch_intervals = 2\n",
        "text_channels = 768\n",
        "neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]\n",
        "neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]\n",
        "base_lr = 2e-3\n",
        "weight_decay = 0.0125\n",
        "train_batch_size_per_gpu = 16\n",
        "# text_model_name = '../pretrained_models/clip-vit-large-patch14-336'\n",
        "text_model_name = 'openai/clip-vit-large-patch14-336'\n",
        "# model settings\n",
        "model = dict(\n",
        "    type='YOLOWorldDetector',\n",
        "    mm_neck=True,\n",
        "    num_train_classes=num_training_classes,\n",
        "    num_test_classes=num_classes,\n",
        "    data_preprocessor=dict(type='YOLOWDetDataPreprocessor'),\n",
        "    backbone=dict(\n",
        "        _delete_=True,\n",
        "        type='MultiModalYOLOBackbone',\n",
        "        image_model={{_base_.model.backbone}},\n",
        "        text_model=dict(\n",
        "            type='HuggingCLIPLanguageBackbone',\n",
        "            model_name=text_model_name,\n",
        "            frozen_modules=['all'])),\n",
        "    neck=dict(type='YOLOWorldPAFPN',\n",
        "              guide_channels=text_channels,\n",
        "              embed_channels=neck_embed_channels,\n",
        "              num_heads=neck_num_heads,\n",
        "              block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),\n",
        "    bbox_head=dict(type='YOLOWorldHead',\n",
        "                   head_module=dict(type='YOLOWorldHeadModule',\n",
        "                                    use_bn_head=True,\n",
        "                                    embed_dims=text_channels,\n",
        "                                    num_classes=num_training_classes)),\n",
        "    train_cfg=dict(assigner=dict(num_classes=num_training_classes)))\n",
        "\n",
        "# dataset settings\n",
        "text_transform = [\n",
        "    dict(type='RandomLoadText',\n",
        "         num_neg_samples=(num_classes, num_classes),\n",
        "         max_num_samples=num_training_classes,\n",
        "         padding_to_max=True,\n",
        "         padding_value=''),\n",
        "    dict(type='mmdet.PackDetInputs',\n",
        "         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',\n",
        "                    'flip_direction', 'texts'))\n",
        "]\n",
        "train_pipeline = [\n",
        "    *_base_.pre_transform,\n",
        "    dict(type='MultiModalMosaic',\n",
        "         img_scale=_base_.img_scale,\n",
        "         pad_val=114.0,\n",
        "         pre_transform=_base_.pre_transform),\n",
        "    dict(\n",
        "        type='YOLOv5RandomAffine',\n",
        "        max_rotate_degree=0.0,\n",
        "        max_shear_degree=0.0,\n",
        "        scaling_ratio_range=(1 - _base_.affine_scale, 1 + _base_.affine_scale),\n",
        "        max_aspect_ratio=_base_.max_aspect_ratio,\n",
        "        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),\n",
        "        border_val=(114, 114, 114)),\n",
        "    *_base_.last_transform[:-1],\n",
        "    *text_transform,\n",
        "]\n",
        "train_pipeline_stage2 = [*_base_.train_pipeline_stage2[:-1], *text_transform]\n",
        "obj365v1_train_dataset = dict(\n",
        "    type='MultiModalDataset',\n",
        "    dataset=dict(\n",
        "        type='YOLOv5Objects365V1Dataset',\n",
        "        data_root='data/objects365v1/',\n",
        "        ann_file='annotations/objects365_train.json',\n",
        "        data_prefix=dict(img='train/'),\n",
        "        filter_cfg=dict(filter_empty_gt=False, min_size=32)),\n",
        "    class_text_path='data/texts/obj365v1_class_texts.json',\n",
        "    pipeline=train_pipeline)\n",
        "\n",
        "mg_train_dataset = dict(type='YOLOv5MixedGroundingDataset',\n",
        "                        data_root='data/mixed_grounding/',\n",
        "                        ann_file='annotations/final_mixed_train_no_coco.json',\n",
        "                        data_prefix=dict(img='gqa/images/'),\n",
        "                        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
        "                        pipeline=train_pipeline)\n",
        "\n",
        "flickr_train_dataset = dict(\n",
        "    type='YOLOv5MixedGroundingDataset',\n",
        "    data_root='data/flickr/',\n",
        "    ann_file='annotations/final_flickr_separateGT_train.json',\n",
        "    data_prefix=dict(img='full_images/'),\n",
        "    filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
        "    pipeline=train_pipeline)\n",
        "\n",
        "train_dataloader = dict(batch_size=train_batch_size_per_gpu,\n",
        "                        collate_fn=dict(type='yolow_collate'),\n",
        "                        dataset=dict(_delete_=True,\n",
        "                                     type='ConcatDataset',\n",
        "                                     datasets=[\n",
        "                                         obj365v1_train_dataset,\n",
        "                                         flickr_train_dataset, mg_train_dataset\n",
        "                                     ],\n",
        "                                     ignore_keys=['classes', 'palette']))\n",
        "\n",
        "test_pipeline = [\n",
        "    *_base_.test_pipeline[:-1],\n",
        "    dict(type='LoadText'),\n",
        "    dict(type='mmdet.PackDetInputs',\n",
        "         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
        "                    'scale_factor', 'pad_param', 'texts'))\n",
        "]\n",
        "coco_val_dataset = dict(\n",
        "    _delete_=True,\n",
        "    type='MultiModalDataset',\n",
        "    dataset=dict(type='YOLOv5LVISV1Dataset',\n",
        "                 data_root='data/coco/',\n",
        "                 test_mode=True,\n",
        "                 ann_file='lvis/lvis_v1_minival_inserted_image_name.json',\n",
        "                 data_prefix=dict(img=''),\n",
        "                 batch_shapes_cfg=None),\n",
        "    class_text_path='data/texts/lvis_v1_class_texts.json',\n",
        "    pipeline=test_pipeline)\n",
        "val_dataloader = dict(dataset=coco_val_dataset)\n",
        "test_dataloader = val_dataloader\n",
        "\n",
        "val_evaluator = dict(type='mmdet.LVISMetric',\n",
        "                     ann_file='data/coco/lvis/lvis_v1_minival_inserted_image_name.json',\n",
        "                     metric='bbox')\n",
        "test_evaluator = val_evaluator\n",
        "\n",
        "# training settings\n",
        "default_hooks = dict(param_scheduler=dict(max_epochs=max_epochs),\n",
        "                     checkpoint=dict(interval=save_epoch_intervals,\n",
        "                                     rule='greater'))\n",
        "custom_hooks = [\n",
        "    dict(type='EMAHook',\n",
        "         ema_type='ExpMomentumEMA',\n",
        "         momentum=0.0001,\n",
        "         update_buffers=True,\n",
        "         strict_load=False,\n",
        "         priority=49),\n",
        "    dict(type='mmdet.PipelineSwitchHook',\n",
        "         switch_epoch=max_epochs - close_mosaic_epochs,\n",
        "         switch_pipeline=train_pipeline_stage2)\n",
        "]\n",
        "train_cfg = dict(max_epochs=max_epochs,\n",
        "                 val_interval=10,\n",
        "                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),\n",
        "                                     _base_.val_interval_stage2)])\n",
        "optim_wrapper = dict(optimizer=dict(\n",
        "    _delete_=True,\n",
        "    type='AdamW',\n",
        "    lr=base_lr,\n",
        "    weight_decay=weight_decay,\n",
        "    batch_size_per_gpu=train_batch_size_per_gpu),\n",
        "                     paramwise_cfg=dict(bias_decay_mult=0.0,\n",
        "                                        norm_decay_mult=0.0,\n",
        "                                        custom_keys={\n",
        "                                            'backbone.text_model':\n",
        "                                            dict(lr_mult=0.01),\n",
        "                                            'logit_scale':\n",
        "                                            dict(weight_decay=0.0)\n",
        "                                        }),\n",
        "                     constructor='YOLOWv5OptimizerConstructor')"
      ],
      "metadata": {
        "id": "2x1_fU_VWGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMPT TUNING"
      ],
      "metadata": {
        "id": "r-UjL55KVH_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_base_ = ('../../third_party/mmyolo/configs/yolov8/'\n",
        "          'yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py')\n",
        "custom_imports = dict(imports=['yolo_world'], allow_failed_imports=False)\n",
        "\n",
        "# hyper-parameters\n",
        "num_classes = 80\n",
        "num_training_classes = 80\n",
        "max_epochs = 80  # Maximum training epochs\n",
        "close_mosaic_epochs = 10\n",
        "save_epoch_intervals = 5\n",
        "text_channels = 512\n",
        "neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]\n",
        "neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]\n",
        "base_lr = 2e-3\n",
        "weight_decay = 0.05\n",
        "train_batch_size_per_gpu = 16\n",
        "load_from = 'pretrained_models/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth'\n",
        "persistent_workers = False\n",
        "\n",
        "# model settings\n",
        "model = dict(type='YOLOWorldPromptDetector',\n",
        "             mm_neck=True,\n",
        "             num_train_classes=num_training_classes,\n",
        "             num_test_classes=num_classes,\n",
        "             embedding_path='embeddings/clip_vit_b32_coco_80_embeddings.npy',\n",
        "             prompt_dim=text_channels,\n",
        "             num_prompts=80,\n",
        "             data_preprocessor=dict(type='YOLOv5DetDataPreprocessor'),\n",
        "             backbone=dict(_delete_=True,\n",
        "                           type='MultiModalYOLOBackbone',\n",
        "                           text_model=None,\n",
        "                           image_model={{_base_.model.backbone}},\n",
        "                           frozen_stages=4,\n",
        "                           with_text_model=False),\n",
        "             neck=dict(type='YOLOWorldPAFPN',\n",
        "                       freeze_all=True,\n",
        "                       guide_channels=text_channels,\n",
        "                       embed_channels=neck_embed_channels,\n",
        "                       num_heads=neck_num_heads,\n",
        "                       block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),\n",
        "             bbox_head=dict(type='YOLOWorldHead',\n",
        "                            head_module=dict(\n",
        "                                type='YOLOWorldHeadModule',\n",
        "                                freeze_all=True,\n",
        "                                use_bn_head=True,\n",
        "                                embed_dims=text_channels,\n",
        "                                num_classes=num_training_classes)),\n",
        "             train_cfg=dict(assigner=dict(num_classes=num_training_classes)))\n",
        "\n",
        "# dataset settings\n",
        "final_transform = [\n",
        "    dict(type='mmdet.PackDetInputs',\n",
        "         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',\n",
        "                    'flip_direction'))\n",
        "]\n",
        "mosaic_affine_transform = [\n",
        "    dict(type='Mosaic',\n",
        "         img_scale=_base_.img_scale,\n",
        "         pad_val=114.0,\n",
        "         pre_transform=_base_.pre_transform),\n",
        "    dict(type='YOLOv5CopyPaste', prob=_base_.copypaste_prob),\n",
        "    dict(\n",
        "        type='YOLOv5RandomAffine',\n",
        "        max_rotate_degree=0.0,\n",
        "        max_shear_degree=0.0,\n",
        "        max_aspect_ratio=100.,\n",
        "        scaling_ratio_range=(1 - _base_.affine_scale, 1 + _base_.affine_scale),\n",
        "        # img_scale is (width, height)\n",
        "        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),\n",
        "        border_val=(114, 114, 114),\n",
        "        min_area_ratio=_base_.min_area_ratio,\n",
        "        use_mask_refine=_base_.use_mask2refine)\n",
        "]\n",
        "train_pipeline = [\n",
        "    *_base_.pre_transform, *mosaic_affine_transform,\n",
        "    dict(type='YOLOv5MixUp',\n",
        "         prob=_base_.mixup_prob,\n",
        "         pre_transform=[*_base_.pre_transform, *mosaic_affine_transform]),\n",
        "    *_base_.last_transform[:-1], *final_transform\n",
        "]\n",
        "\n",
        "train_pipeline_stage2 = [*_base_.train_pipeline_stage2[:-1], *final_transform]\n",
        "\n",
        "coco_train_dataset = dict(type='YOLOv5CocoDataset',\n",
        "                          data_root='data/coco',\n",
        "                          ann_file='annotations/instances_train2017.json',\n",
        "                          data_prefix=dict(img='train2017/'),\n",
        "                          filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
        "                          pipeline=train_pipeline)\n",
        "\n",
        "train_dataloader = dict(persistent_workers=persistent_workers,\n",
        "                        batch_size=train_batch_size_per_gpu,\n",
        "                        collate_fn=dict(type='yolow_collate'),\n",
        "                        dataset=coco_train_dataset)\n",
        "\n",
        "train_dataloader = dict(persistent_workers=persistent_workers,\n",
        "                        batch_size=train_batch_size_per_gpu,\n",
        "                        collate_fn=dict(type='yolow_collate'),\n",
        "                        dataset=coco_train_dataset)\n",
        "test_pipeline = [\n",
        "    *_base_.test_pipeline[:-1],\n",
        "    dict(type='mmdet.PackDetInputs',\n",
        "         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
        "                    'scale_factor', 'pad_param'))\n",
        "]\n",
        "coco_val_dataset = dict(type='YOLOv5CocoDataset',\n",
        "                        data_root='data/coco',\n",
        "                        ann_file='annotations/instances_val2017.json',\n",
        "                        data_prefix=dict(img='val2017/'),\n",
        "                        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
        "                        pipeline=test_pipeline)\n",
        "\n",
        "val_dataloader = dict(dataset=coco_val_dataset)\n",
        "test_dataloader = val_dataloader\n",
        "# training settings\n",
        "default_hooks = dict(param_scheduler=dict(scheduler_type='linear',\n",
        "                                          lr_factor=0.01,\n",
        "                                          max_epochs=max_epochs),\n",
        "                     checkpoint=dict(max_keep_ckpts=-1,\n",
        "                                     save_best=None,\n",
        "                                     interval=save_epoch_intervals))\n",
        "custom_hooks = [\n",
        "    dict(type='EMAHook',\n",
        "         ema_type='ExpMomentumEMA',\n",
        "         momentum=0.0001,\n",
        "         update_buffers=True,\n",
        "         strict_load=False,\n",
        "         priority=49),\n",
        "    dict(type='mmdet.PipelineSwitchHook',\n",
        "         switch_epoch=max_epochs - close_mosaic_epochs,\n",
        "         switch_pipeline=train_pipeline_stage2)\n",
        "]\n",
        "train_cfg = dict(max_epochs=max_epochs,\n",
        "                 val_interval=5,\n",
        "                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),\n",
        "                                     _base_.val_interval_stage2)])\n",
        "optim_wrapper = dict(optimizer=dict(\n",
        "    _delete_=True,\n",
        "    type='AdamW',\n",
        "    lr=base_lr,\n",
        "    weight_decay=weight_decay,\n",
        "    batch_size_per_gpu=train_batch_size_per_gpu),\n",
        "                     paramwise_cfg=dict(bias_decay_mult=0.0,\n",
        "                                        norm_decay_mult=0.0,\n",
        "                                        custom_keys={\n",
        "                                            'backbone.text_model':\n",
        "                                            dict(lr_mult=0.01),\n",
        "                                            'logit_scale':\n",
        "                                            dict(weight_decay=0.0),\n",
        "                                            'embeddings':\n",
        "                                            dict(weight_decay=0.0)\n",
        "                                        }),\n",
        "                     constructor='YOLOWv5OptimizerConstructor')\n",
        "\n",
        "# evaluation settings\n",
        "val_evaluator = dict(_delete_=True,\n",
        "                     type='mmdet.CocoMetric',\n",
        "                     proposal_nums=(100, 1, 10),\n",
        "                     ann_file='data/coco/annotations/instances_val2017.json',\n",
        "                     metric='bbox')\n",
        "find_unused_parameters = True"
      ],
      "metadata": {
        "id": "vKrlc1_OVFhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# added image demo\n",
        "_base_ = ('../../third_party/mmyolo/configs/yolov8/'\n",
        "          'yolov8_l_syncbn_fast_8xb16-500e_coco.py')\n",
        "custom_imports = dict(imports=['yolo_world'], allow_failed_imports=False)\n",
        "\n",
        "# hyper-parameters\n",
        "num_classes = 80\n",
        "num_training_classes = 80\n",
        "max_epochs = 80  # Maximum training epochs\n",
        "close_mosaic_epochs = 10\n",
        "save_epoch_intervals = 5\n",
        "text_channels = 512\n",
        "neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]\n",
        "neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]\n",
        "base_lr = 2e-4\n",
        "weight_decay = 0.05\n",
        "train_batch_size_per_gpu = 16\n",
        "load_from = 'pretrained_models/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth'\n",
        "persistent_workers = False\n",
        "\n",
        "# model settings\n",
        "model = dict(type='YOLOWorldPromptDetector',\n",
        "             mm_neck=True,\n",
        "             num_train_classes=num_training_classes,\n",
        "             num_test_classes=num_classes,\n",
        "             embedding_path='embeddings/clip_vit_b32_coco_80_embeddings.npy',\n",
        "             prompt_dim=text_channels,\n",
        "             num_prompts=80,\n",
        "             freeze_prompt=False,\n",
        "             data_preprocessor=dict(type='YOLOv5DetDataPreprocessor'),\n",
        "             backbone=dict(_delete_=True,\n",
        "                           type='MultiModalYOLOBackbone',\n",
        "                           text_model=None,\n",
        "                           image_model={{_base_.model.backbone}},\n",
        "                           frozen_stages=4,\n",
        "                           with_text_model=False),\n",
        "             neck=dict(type='YOLOWorldPAFPN',\n",
        "                       freeze_all=True,\n",
        "                       guide_channels=text_channels,\n",
        "                       embed_channels=neck_embed_channels,\n",
        "                       num_heads=neck_num_heads,\n",
        "                       block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),\n",
        "             bbox_head=dict(type='YOLOWorldHead',\n",
        "                            head_module=dict(\n",
        "                                type='YOLOWorldHeadModule',\n",
        "                                freeze_all=True,\n",
        "                                use_bn_head=True,\n",
        "                                embed_dims=text_channels,\n",
        "                                num_classes=num_training_classes)),\n",
        "             train_cfg=dict(assigner=dict(num_classes=num_training_classes)))\n",
        "\n",
        "# dataset settings\n",
        "coco_train_dataset = dict(type='YOLOv5CocoDataset',\n",
        "                          data_root='data/coco',\n",
        "                          ann_file='annotations/instances_train2017.json',\n",
        "                          data_prefix=dict(img='train2017/'),\n",
        "                          filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
        "                          pipeline=_base_.train_pipeline)\n",
        "\n",
        "train_dataloader = dict(persistent_workers=persistent_workers,\n",
        "                        batch_size=train_batch_size_per_gpu,\n",
        "                        collate_fn=dict(type='yolow_collate'),\n",
        "                        dataset=coco_train_dataset)\n",
        "\n",
        "coco_val_dataset = dict(type='YOLOv5CocoDataset',\n",
        "                        data_root='data/coco',\n",
        "                        ann_file='annotations/instances_val2017.json',\n",
        "                        data_prefix=dict(img='val2017/'),\n",
        "                        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
        "                        pipeline=_base_.test_pipeline)\n",
        "\n",
        "val_dataloader = dict(dataset=coco_val_dataset)\n",
        "test_dataloader = val_dataloader\n",
        "# training settings\n",
        "default_hooks = dict(param_scheduler=dict(scheduler_type='linear',\n",
        "                                          lr_factor=0.01,\n",
        "                                          max_epochs=max_epochs),\n",
        "                     checkpoint=dict(max_keep_ckpts=-1,\n",
        "                                     save_best=None,\n",
        "                                     interval=save_epoch_intervals))\n",
        "custom_hooks = [\n",
        "    dict(type='EMAHook',\n",
        "         ema_type='ExpMomentumEMA',\n",
        "         momentum=0.0001,\n",
        "         update_buffers=True,\n",
        "         strict_load=False,\n",
        "         priority=49),\n",
        "    dict(type='mmdet.PipelineSwitchHook',\n",
        "         switch_epoch=max_epochs - close_mosaic_epochs,\n",
        "         switch_pipeline=_base_.train_pipeline_stage2)\n",
        "]\n",
        "train_cfg = dict(max_epochs=max_epochs,\n",
        "                 val_interval=5,\n",
        "                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),\n",
        "                                     _base_.val_interval_stage2)])\n",
        "\n",
        "optim_wrapper = dict(optimizer=dict(\n",
        "    _delete_=True,\n",
        "    type='AdamW',\n",
        "    lr=base_lr,\n",
        "    weight_decay=weight_decay,\n",
        "    batch_size_per_gpu=train_batch_size_per_gpu),\n",
        "                     paramwise_cfg=dict(custom_keys={\n",
        "                                            'backbone.text_model':\n",
        "                                            dict(lr_mult=0.01),\n",
        "                                            'logit_scale':\n",
        "                                            dict(weight_decay=0.0),\n",
        "                                            'embeddings':\n",
        "                                            dict(weight_decay=0.0)\n",
        "                                        }),\n",
        "                     constructor='YOLOWv5OptimizerConstructor')\n",
        "\n",
        "# evaluation settings\n",
        "val_evaluator = dict(_delete_=True,\n",
        "                     type='mmdet.CocoMetric',\n",
        "                     proposal_nums=(100, 1, 10),\n",
        "                     ann_file='data/coco/annotations/instances_val2017.json',\n",
        "                     metric='bbox')"
      ],
      "metadata": {
        "id": "ZsbHE7DDW62m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "u7q1mJfiWJaN",
        "r-UjL55KVH_-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}